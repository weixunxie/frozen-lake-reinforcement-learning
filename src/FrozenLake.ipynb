{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment CustomFrozenLake-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 2 2 2 0]\n",
      " [0 0 0 0 0]]\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym \n",
    "import time\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "\n",
    "class CustomFrozenLakeEnv(FrozenLakeEnv):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomFrozenLakeEnv, self).__init__(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Perform the action\n",
    "        observations, reward, terminated, truncated, info = super(CustomFrozenLakeEnv, self).step(action)\n",
    "        # If the episode is done and the reward is 0, it means we fell in a hole\n",
    "        if terminated and reward == 0:\n",
    "            reward = -1  # Change the reward for falling in a hole to -1\n",
    "        return observations, reward, terminated, truncated, info\n",
    "\n",
    "# To use the custom environment, you need to register it with Gym\n",
    "from gymnasium.envs.registration import register, registry\n",
    "\n",
    "# Check if the environment is already registered to avoid duplication errors\n",
    "if True:#'CustomFrozenLake-v0' not in registry.env_specs:\n",
    "    register(\n",
    "        id='CustomFrozenLake-v0',\n",
    "        entry_point=CustomFrozenLakeEnv,\n",
    "    )\n",
    "\n",
    "# Now you can create and use your custom environment\n",
    "#env = gym.make('CustomFrozenLake-v0')\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "\"\"\"\n",
    "For policy_evaluation, policy_improvement, policy_iteration and valu2wsws:\n",
    "\n",
    "    P (dict): From gym.core.Environment\n",
    "        For each pair of states in [0, nS - 1] and actions in [0, nA - 1], P[state][action] is a\n",
    "        list of tuples of the form [(probability, nextstate, reward, terminal),...] where\n",
    "            - probability: float\n",
    "                the probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "            - nextstate: int\n",
    "                denotes the state we transition to (in range [0, nS - 1])\n",
    "            - reward: int\n",
    "                either 0 or 1, the reward for transitioning from \"state\" to\n",
    "                \"nextstate\" with \"action\"\n",
    "            - terminal: bool\n",
    "              True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "    nS (int): number of states in the environment\n",
    "    nA (int): number of actions in the environment\n",
    "    gamma (float): Discount factor. Number in range [0, 1)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "gym.make('FrozenLake-v1', desc=generate_random_map(size=8))\n",
    "\n",
    "    \"4x4\":[\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "        ]\n",
    "\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\",\n",
    "    ]\n",
    "\n",
    "If desc=None: map_name\n",
    "If desc=None and map_name=None: desc=generate_random_map(size=8) (80% of S is frozen)\n",
    "If is_slippery: P(a)=1/3, P(a+1)=1/3, P(a-1)=1/3\n",
    "\"\"\"\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Evaluate the value function from a given policy.\n",
    "\n",
    "    Args:\n",
    "            P, nS, nA, gamma: defined at beginning of file\n",
    "            policy (np.array[nS]): The policy to evaluate. Maps states to actions.\n",
    "            tol (float): Terminate policy evaluation when\n",
    "                    max |value_function(s) - prev_value_function(s)| < tol\n",
    "\n",
    "    Returns:\n",
    "            value_function (np.ndarray[nS]): The value function of the given policy, where value_function[s] is\n",
    "                    the value of state s.\n",
    "    \"\"\"\n",
    "\n",
    "    V = np.zeros(nS)\n",
    "    pi = policy    \n",
    "    \n",
    "    d = 1\n",
    "\n",
    "    while d > tol:\n",
    "        V_ = np.zeros(nS)\n",
    "        for s in range(nS):\n",
    "             a = pi[s]\n",
    "             for p,s_,r,_ in P[s][a]:\n",
    "                V_[s] += p*(r+gamma*V[s_])\n",
    "        d = np.max(np.abs(V_ - V))\n",
    "        V = V_\n",
    "    \n",
    "    return V\n",
    "\n",
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Given the value function from policy improve the policy.\n",
    "\n",
    "    Args:\n",
    "            P, nS, nA, gamma: defined at beginning of file\n",
    "            value_from_policy (np.ndarray): The value calculated from the policy\n",
    "            policy (np.array): The previous policy\n",
    "\n",
    "    Returns:\n",
    "            new_policy (np.ndarray[nS]): An array of integers. Each integer is the optimal\n",
    "            action to take in that state according to the environment dynamics and the\n",
    "            given value function.\n",
    "    \"\"\"\n",
    "\n",
    "    pi = np.zeros(nS, dtype=\"int\")\n",
    "    V = value_from_policy\n",
    "    flag = True\n",
    "    \n",
    "    for s in range(nS):\n",
    "    \n",
    "        a_0 = pi[s]\n",
    "        \n",
    "        A = np.zeros(nA)\n",
    "        \n",
    "        for a in range(nA):\n",
    "            temp = 0\n",
    "            for p, s_, r, _ in P[s][a]:\n",
    "                temp += p*(r+gamma*V[s_])\n",
    "            A[a] = temp\n",
    "        pi[s] = np.argmax(A)\n",
    "            \n",
    "    return pi\n",
    "\n",
    "#0X0\n",
    "# policy iteration\n",
    "\n",
    "\n",
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    Args:\n",
    "            P, nS, nA, gamma: defined at beginning of file\n",
    "            tol (float): tol parameter used in policy_evaluation()\n",
    "\n",
    "    Returns:\n",
    "            value_function (np.ndarray[nS]): value function resulting from policy iteration\n",
    "            policy (np.ndarray[nS]): policy resulting from policy iteration\n",
    "\n",
    "    Hint:\n",
    "            You should call the policy_evaluation() and policy_improvement() methods to\n",
    "            implement this method.\n",
    "    \"\"\"\n",
    "\n",
    "    V = np.zeros(nS)\n",
    "    pi = np.zeros(nS, dtype=int)\n",
    "    \n",
    "    flag = True\n",
    "    \n",
    "    while flag:\n",
    "        V = policy_evaluation(P,nS,nA,pi,gamma,tol)\n",
    "        pi_ = policy_improvement(P, nS, nA, V, pi, gamma)\n",
    "        d = pi_ - pi\n",
    "        \n",
    "        if np.linalg.norm(d) < tol:\n",
    "            flag = False\n",
    "        pi = pi_\n",
    "    \n",
    "    return V, pi\n",
    "\n",
    "\n",
    "#value iteration\n",
    "\n",
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Learn value function and policy by using value iteration method for a given\n",
    "    gamma and environment\n",
    "\n",
    "    Args:\n",
    "            P, nS, nA, gamma: defined at beginning of file\n",
    "            tol (float): Terminate value iteration when\n",
    "                            max |value_function(s) - prev_value_function(s)| < tol\n",
    "\n",
    "    Returns:\n",
    "            value_function (np.ndarray[nS]): value function resulting from value iteration\n",
    "            policy (np.ndarray[nS]): policy resulting from value iteration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    V = np.zeros(nS)\n",
    "    pi = np.zeros(nS, dtype=int)\n",
    "    \n",
    "    d = 1\n",
    "    while d >= tol:\n",
    "        V_ = np.zeros(nS)\n",
    "        for s in range(nS):\n",
    "            A = np.zeros(nA)\n",
    "            for a in range(nA):\n",
    "                for p,s_,r,_ in P[s][a]:\n",
    "                    A[a] += p*(r+gamma*V[s_])\n",
    "            V_[s] = max(A)\n",
    "        d = np.linalg.norm(V_ - V)\n",
    "        V = V_\n",
    "    \n",
    "    for s in range(nS):\n",
    "        A = np.zeros(nA)\n",
    "        for a in range(nA):\n",
    "            for p,s_,r,_ in P[s][a]:\n",
    "                A[a] += p*(r+gamma*V[s_])\n",
    "            Amax = np.argmax(A)\n",
    "        pi[s] = Amax\n",
    "       \n",
    "    return V, pi\n",
    "\n",
    "def q_learning(env, nS, nA, gamma=0.9, alpha=0.9, epsilon=1, decay = 0.95, epochs = 100, max_steps=10):\n",
    "    \"\"\"\n",
    "    Learn Q function and policy by using Q learning for a given\n",
    "    gamma, alpha, epsilon and environment\n",
    "\n",
    "    Args:\n",
    "            env, nS, nA, gamma: defined at beginning of file\n",
    "            alpha: learning rate\n",
    "            epsilon: for epsilon greedy algorithm\n",
    "            tol (float): Terminate value iteration when\n",
    "                            max |value_function(s) - prev_value_function(s)| < tol\n",
    "\n",
    "    Returns:\n",
    "            Q (np.ndarray([nS,nA])): Q function resulting from Q learning\n",
    "            pi (np.ndarray([nS])): policy resulting from Q iteration\n",
    "\n",
    "    \"\"\"\n",
    "    Q = np.zeros([nS,nA])\n",
    "    pi = rng.integers(low=0, high=4, size=nS)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        s, _ = env.reset()\n",
    "        epsilon = epsilon*decay\n",
    "        for _ in range(max_steps):\n",
    "            coin = rng.random()\n",
    "            if coin < 1 - epsilon:\n",
    "                a = np.argmax(Q[s])\n",
    "            else:\n",
    "                a = rng.integers(4)\n",
    "            s_, r, terminated, truncated, _ = env.step(a) \n",
    "            Q[s,a] = (1-alpha)*Q[s,a] + alpha*(r+gamma*np.max(Q[s_]))\n",
    "            s = s_\n",
    "            if truncated or terminated:\n",
    "                break\n",
    "\n",
    "    for s in range(nS):\n",
    "        pi[s] = np.argmax(Q[s])\n",
    "       \n",
    "    return Q, pi\n",
    "\n",
    "def render_single(env, pi, max_steps=50, delay=1):\n",
    "    \"\"\"\n",
    "    Renders a single game given environment, policy and maximum number of steps.\n",
    "    \n",
    "    Args: \n",
    "            env: environment\n",
    "            pi: policy\n",
    "            max_steps: maximum number of steps to reach goal\n",
    "    Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    obs, info = env.reset()\n",
    "    for _ in range(max_steps):\n",
    "        env.render()\n",
    "        time.sleep(delay)\n",
    "        a = pi[obs]  \n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        R += r\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    env.render()\n",
    "    time.sleep(delay)\n",
    "    if not (terminated or truncated):\n",
    "        print(\"Anna did not reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % R)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Create custom map\n",
    "    d = ['HHHHH',\n",
    "         'HSFFG',\n",
    "         'HHHHH']\n",
    "    \n",
    "    #Define the environment\n",
    "    env = gym.make('CustomFrozenLake-v0', desc=d, map_name=\"4x4\", is_slippery=False)#, render_mode=\"human\")\n",
    "\n",
    "    #Find policy using policy iteration\n",
    "    #V_pi, p_pi = policy_iteration(env.P, env.observation_space.n, env.action_space.n)\n",
    "    #render_single(env, p_pi)\n",
    "\n",
    "    #Find policy using value iteration\n",
    "    #V_vi, p_vi = value_iteration(env.P, env.observation_space.n, env.action_space.n)\n",
    "    #render_single(env, p_vi)\n",
    "\n",
    "    #Find policy using Q-learning\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    Q, pi = q_learning(env, env.observation_space.n, env.action_space.n, decay = 0.99, max_steps=50, epochs = 1000) \n",
    "\n",
    "    #Print the final policy generated by Q\n",
    "    print(pi.reshape(3,5))\n",
    "\n",
    "    #Render a single game\n",
    "    env = gym.make('CustomFrozenLake-v0', desc=d, map_name=\"4x4\", is_slippery=False, render_mode=\"human\") \n",
    "    render_single(env, pi, delay = 1, max_steps=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
