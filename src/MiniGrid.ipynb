{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `MiniGrid-Empty-5x5` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1y/42jcd32x74sfkj7xjlx937xm0000gn/T/ipykernel_1613/2607488803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m#Define the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MiniGrid-Empty-5x5-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m#Find policy using Q-learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0menv_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0menv_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         raise error.Error(\n\u001b[1;32m    529\u001b[0m             \u001b[0;34mf\"No registered env with id: {env_name}. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0msuggestion_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\" Did you mean: `{suggestion[0]}`?\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     raise error.NameNotFound(\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0;34mf\"Environment `{name}` doesn't exist{namespace_msg}.{suggestion_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment `MiniGrid-Empty-5x5` doesn't exist."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym \n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "\"\"\"\n",
    "For Q-learning\n",
    "\n",
    "    P (dict): From gym.core.Environment\n",
    "        For each pair of states in [0, nS - 1] and actions in [0, nA - 1], P[state][action] is a\n",
    "        list of tuples of the form [(probability, nextstate, reward, terminal),...] where\n",
    "            - probability: float\n",
    "                the probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "            - nextstate: int\n",
    "                denotes the state we transition to (in range [0, nS - 1])\n",
    "            - reward: int\n",
    "                either 0 or 1, the reward for transitioning from \"state\" to\n",
    "                \"nextstate\" with \"action\"\n",
    "            - terminal: bool\n",
    "              True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "    nS (int): number of states in the environment\n",
    "    nA (int): number of actions in the environment\n",
    "    gamma (float): Discount factor. Number in range [0, 1)\n",
    "\"\"\"\n",
    "\n",
    "def pre(obs):\n",
    "    #maps observations to tuples (x,d) where x and d are real numbers and d represents the direction\n",
    "    return tuple(list(obs['image'][:,:,0].flatten())+[obs['direction']])\n",
    "\n",
    "def q_learning(env, nS=50, nA=3, gamma=0.9, alpha=0.9, epsilon=1, decay = 0.95, epochs = 100, max_steps=10):\n",
    "    \"\"\"\n",
    "    Learn Q function and policy by using Q learning for a given\n",
    "    gamma, alpha, epsilon and environment\n",
    "\n",
    "    Args:\n",
    "            env, nS, nA, gamma: defined at beginning of file\n",
    "            alpha(float): learning rate\n",
    "            epsilon(float): for epsilon greedy algorithm\n",
    "            tol (float): Terminate value iteration when\n",
    "                            max |value_function(s) - prev_value_function(s)| < tol\n",
    "\n",
    "    Returns:\n",
    "            Q (np.ndarray([nS,nA])): Q function resulting from Q learning\n",
    "            pi (np.ndarray([nS])): policy resulting from Q iteration\n",
    "\n",
    "    \"\"\"\n",
    "    # Keep track of states\n",
    "    bag = defaultdict()\n",
    "    i = 0\n",
    "\n",
    "    # Initialize Q function and policy pi\n",
    "    Q = np.zeros([nS,nA])\n",
    "    pi = rng.integers(low=0, high=4, size=nS)\n",
    "\n",
    "    # Run episodes of Q-learning\n",
    "    for _ in range(epochs):\n",
    "        obs, _ = env.reset()\n",
    "        obs = pre(obs)\n",
    "        if obs in bag:\n",
    "            s = bag[obs]\n",
    "        else:\n",
    "            bag[obs]=i\n",
    "            s = i \n",
    "            i += 1\n",
    "        epsilon = epsilon*decay\n",
    "        #max_steps = (max_steps*decay)//1\n",
    "        for _ in range(max_steps):\n",
    "            coin = rng.random()\n",
    "            if coin < 1 - epsilon:\n",
    "                a = np.argmax(Q[s])\n",
    "            else:\n",
    "                a = rng.integers(nA)\n",
    "            obs, r, terminated, truncated, _ = env.step(a) \n",
    "            obs = pre(obs)\n",
    "            if obs in bag:\n",
    "                s_ = bag[obs]\n",
    "            else:\n",
    "                bag[obs]=i\n",
    "                s_ = i \n",
    "                i += 1\n",
    "            Q[s,a] = (1-alpha)*Q[s,a] + alpha*(r+gamma*np.max(Q[s_]))\n",
    "            s = s_\n",
    "            if truncated or terminated:\n",
    "                break\n",
    "\n",
    "    # Generate greedy policy on Q\n",
    "    for s in range(nS):\n",
    "        pi[s] = np.argmax(Q[s])\n",
    "       \n",
    "    return Q, pi\n",
    "\n",
    "def render_single(env, pi, gamma = 0.9, max_steps=50, delay=1):\n",
    "    \"\"\"\n",
    "    Renders a single game given environment, policy and maximum number of steps.\n",
    "    \n",
    "    Args: \n",
    "            env: environment\n",
    "            pi: policy\n",
    "            max_steps: maximum number of steps to reach goal\n",
    "    Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    obs, info = env.reset()\n",
    "    for i in range(max_steps):\n",
    "        env.render()\n",
    "        time.sleep(delay)\n",
    "        a = pi[obs]  \n",
    "        obs, r, terminated, truncated, info = env.step(a)\n",
    "        R += r*(gamma**i)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    env.render()\n",
    "    time.sleep(delay)\n",
    "    if not (terminated or truncated):\n",
    "        print(\"Anna did not reach a terminal state in {} steps.\".format(max_steps))\n",
    "    else:\n",
    "        print(\"Episode reward: %f\" % R)\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #Define the environment\n",
    "    env = gym.make('MiniGrid-Empty-5x5-v0', render_mode=\"human\")\n",
    "\n",
    "    #Find policy using Q-learning\n",
    "    #nS, nA = env.observation_space.n, env.action_space.n\n",
    "    Q, pi = q_learning(env, decay = 0.95, max_steps=50, epochs = 50) \n",
    "\n",
    "    #Render a single game\n",
    "    #render_single(env, pi, delay = 0, max_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
